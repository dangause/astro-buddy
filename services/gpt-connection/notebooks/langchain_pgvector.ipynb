{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Run export OPENAI_API_KEY=sk-YOUR_OPENAI_API_KEY...\n",
    "# Get openAI api key by reading local .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "dotenv_path = os.path.abspath('../../../.env')\n",
    "_ = load_dotenv(dotenv_path)\n",
    "OPENAI_API_KEY  = os.environ['OPENAI_API_KEY']\n",
    "PG_DB_PW = os.environ['POSTGRES_DB_PASSWORD']\n",
    "host= os.environ['POSTGRES_DB_HOST']\n",
    "port= os.environ['POSTGRES_DB_PORT']\n",
    "user= os.environ['POSTGRES_DB_USER']\n",
    "password= os.environ['POSTGRES_DB_PASSWORD']\n",
    "dbname= os.environ['POSTGRES_DB_DBNAME']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s4/l2klswfj1wvcw_vbmvvbc70h0000gn/T/ipykernel_60761/3406201478.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tiktoken\n",
    "import psycopg2\n",
    "import ast\n",
    "import pgvector\n",
    "import math\n",
    "from psycopg2.extras import execute_values\n",
    "from pgvector.psycopg2 import register_vector\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.vectorstores.pgvector import PGVector\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONNECTION_STRING = f\"postgresql://{user}:{password}@{host}:{port}/{dbname}\"\n",
    "# CONNECTION_STRING = f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{dbname}?sslmode=require\"\n",
    "CONNECTION_STRING = f\"postgresql://{user}:{password}@{host}:{port}/{dbname}\"\n",
    "collection_name= \"arxiv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=host,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    password=PG_DB_PW,\n",
    "    database=dbname\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to PostgreSQL database in Timescale using connection string\n",
    "conn = psycopg2.connect(CONNECTION_STRING)\n",
    "cur = conn.cursor()\n",
    "\n",
    "#install pgvector\n",
    "cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\");\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df = pd.read_csv('../../data-ingest/data/embeddings/test_embedding.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load documents from Pandas dataframe for insertion into database\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "# page_content_column is the column name in the dataframe to create embeddings for\n",
    "loader = DataFrameLoader(embedding_df, page_content_column = 'content')\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAIEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OpenAIEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pgvector import DistanceStrategy\n",
    "\n",
    "db = PGVector.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    distance_strategy=DistanceStrategy.COSINE,\n",
    "    pre_delete_collection=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PGVector.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# Query for which we want to find semantically similar documents\n",
    "query = \"What is a quasar?\"\n",
    "\n",
    "#Fetch the k=3 most similar documents\n",
    "docs =  db.similarity_search(query, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from database\n",
    "# We specify the number of results we want to retrieve (k=3)\n",
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\"k\": 3}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0, model = 'gpt-3.5-turbo-16k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"What is a quasar?\"\n",
    "\n",
    "response = qa_stuff.run(query)\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New chain to return context and sources\n",
    "qa_stuff_with_sources = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "query =  \"What is the evidence for an enhanced accretion episode from Sgr A* in 2019?\"\n",
    "\n",
    "# To run the query, we use a different syntax since we're returning more than just the response text\n",
    "responses = qa_stuff_with_sources({\"query\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_documents = responses[\"source_documents\"]\n",
    "source_content = [doc.page_content for doc in source_documents]\n",
    "source_metadata = [doc.metadata for doc in source_documents]\n",
    "\n",
    "# Construct a single string with the LLM output and the source titles and urls\n",
    "def construct_result_with_sources():\n",
    "    result = responses['result']\n",
    "    result += \"\\n\\n\"\n",
    "    result += \"Sources used:\"\n",
    "    for i in range(len(source_content)):    \n",
    "        result += \"\\n\\n\"\n",
    "        result += source_metadata[i]['title']\n",
    "        result += \"\\n\\n\"\n",
    "        return result\n",
    "\n",
    "display(Markdown(construct_result_with_sources()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  \"What distinguishes a quasar from a normal black hole?\"\n",
    "\n",
    "# To run the query, we use a different syntax since we're returning more than just the response text\n",
    "responses = qa_stuff_with_sources({\"query\": query})\n",
    "\n",
    "source_documents = responses[\"source_documents\"]\n",
    "source_content = [doc.page_content for doc in source_documents]\n",
    "source_metadata = [doc.metadata for doc in source_documents]\n",
    "\n",
    "# Construct a single string with the LLM output and the source titles and urls\n",
    "def construct_result_with_sources():\n",
    "    result = responses['result']\n",
    "    result += \"\\n\\n\"\n",
    "    result += \"Sources used:\"\n",
    "    for i in range(len(source_content)):    \n",
    "        result += \"\\n\\n\"\n",
    "        result += source_metadata[i]['title']\n",
    "        result += \"\\n\\n\"\n",
    "        return result\n",
    "\n",
    "display(Markdown(construct_result_with_sources()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = PGVector.from_documents(\n",
    "#     embedding=embeddings,\n",
    "#     documents=docs,\n",
    "#     collection_name=collection_name,\n",
    "#     connection_string=CONNECTION_STRING,\n",
    "#     pre_delete_collection=False,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = PGVector(\n",
    "    collection_name=collection_name,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_texts_df = pd.read_csv('../../data-ingest/data/text/pdf_texts.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Detection of stellar light from quasar host ga...\n",
       "1    Quasars and the\\nIntergalactic Medium at\\nCosm...\n",
       "2    Draft version August 22, 2023\\nTypeset using L...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_texts_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['06f9f18a-bfa6-11ee-8a3d-9801a78f9833',\n",
       " '06f9fdf6-bfa6-11ee-8a3d-9801a78f9833',\n",
       " '06f9fee6-bfa6-11ee-8a3d-9801a78f9833']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.add_texts(pdf_texts_df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"article_title\": {\"type\": \"string\"},\n",
    "        \"authors\": {\"type\": \"string\"},\n",
    "        \"publish_date\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"article_title\", \"authors\", \"publish_date\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['113d7142-bfa8-11ee-8a3d-9801a78f9833',\n",
       " '113d72c8-bfa8-11ee-8a3d-9801a78f9833',\n",
       " '113d7340-bfa8-11ee-8a3d-9801a78f9833']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.add_texts(pdf_texts_df['content'], metadatas=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = store.as_retriever()\n",
    "llm = ChatOpenAI(temperature = 0.0, model = 'gpt-3.5-turbo-16k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New chain to return context and sources\n",
    "qa_stuff_with_sources = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "query =  \"What is the evidence for an enhanced accretion episode from Sgr A* in 2019?\"\n",
    "\n",
    "# To run the query, we use a different syntax since we're returning more than just the response text\n",
    "responses = qa_stuff_with_sources({\"query\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_documents = responses[\"source_documents\"]\n",
    "source_content = [doc.page_content for doc in source_documents]\n",
    "source_metadata = [doc.metadata for doc in source_documents]\n",
    "\n",
    "# Construct a single string with the LLM output and the source titles and urls\n",
    "def construct_result_with_sources():\n",
    "    result = responses['result']\n",
    "    result += \"\\n\\n\"\n",
    "    result += \"Sources used:\"\n",
    "    for i in range(len(source_content)):    \n",
    "        result += \"\\n\\n\"\n",
    "        result += source_metadata[i]['title']\n",
    "        result += \"\\n\\n\"\n",
    "        return result\n",
    "\n",
    "display(Markdown(construct_result_with_sources()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
